{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "3c6e1579-e70e-4655-ef0d-c54690b5822d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.3.18: Fast Llama patching. Transformers: 4.50.0.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = torch.half,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "5cce5b38-1196-4c66-b7b8-468be8298700"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.18 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "21vrJ5uUw4Sl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "K5iH3oykYT7S"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "d1chlz-SLeZQ"
   },
   "outputs": [],
   "source": [
    "function_df = pd.read_csv(\"out.csv\")\n",
    "alpaca_df = pd.read_csv(\"out_alpaca.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0yjiUho8CAQq"
   },
   "outputs": [],
   "source": [
    "en_dataset = load_dataset('json', data_files={\"Salesforce/xlam-function-calling-60k\"}, split = \"train\")\n",
    "en_dataset = en_dataset.remove_columns([\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "bWmez1c3t2Yw",
    "outputId": "3b9cb415-fd0a-4b18-f212-5d134adffe51"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Where can I find live giveaways for beta access and games?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_dataset[\"query\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "L7m5cL8t5cKN"
   },
   "outputs": [],
   "source": [
    "NUM_SAMPLES_TOTAL = 10000 #tr_dataset.num_rows\n",
    "NUM_SAMPLES_TRAIN = int(NUM_SAMPLES_TOTAL * 0.9)\n",
    "NUM_SAMPLES_INSTRUCT = int(NUM_SAMPLES_TRAIN * 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nuRTnezQMQJ6"
   },
   "outputs": [],
   "source": [
    "function_indices = function_df[\"2\"].nlargest(NUM_SAMPLES_TOTAL).index\n",
    "insruction_indices = alpaca_df[\"2\"].nlargest(NUM_SAMPLES_INSTRUCT).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "A83PpDPkM8B2"
   },
   "outputs": [],
   "source": [
    "function_calling_dataset = en_dataset.select(function_indices)\n",
    "function_calling_dataset = function_calling_dataset.shuffle(seed=42)\n",
    "function_calling_train = function_calling_dataset.select(range(NUM_SAMPLES_TRAIN))\n",
    "function_calling_validation = function_calling_dataset.select(range(NUM_SAMPLES_TRAIN, NUM_SAMPLES_TOTAL)) #validation is only function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SF7ly3qJHDDy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "13QAsEYkCrc-"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "import json\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    query = examples[\"query\"]\n",
    "    answer = examples[\"answers\"]\n",
    "    convos = [{\"role\":\"user\",\"content\":query},{\"role\":\"assistant\",\"content\":answer}]\n",
    "    tool_object = json.loads(examples[\"tools\"])\n",
    "    texts = tokenizer.apply_chat_template(convos,tools=tool_object,tokenize = False, add_generation_prompt = False)\n",
    "    texts = texts.replace('\"parameters\": d','\"arguments\": d') # original tool use function uses parameters our dataset uses arguments\n",
    "\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "function_calling_train= function_calling_train.map(formatting_prompts_func, batched = False,)\n",
    "function_calling_validation = function_calling_validation.map(formatting_prompts_func, batched = False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "uKsJg4j_LGFX",
    "outputId": "04202764-d9ff-4520-e1bd-a70d3e374db1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nEnvironment: ipython\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nGiven the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\\n\\nRespond in the format {\"name\": function name, \"arguments\": dictionary of argument name and its value}.Do not use variables.\\n\\n{\\n    \"name\": \"get_wisdom_quotes_by_topic\",\\n    \"description\": \"Fetches wisdom quotes related to a specific topic using the RapidAPI service.\",\\n    \"parameters\": {\\n        \"topic\": {\\n            \"description\": \"The topic to retrieve wisdom quotes about.\",\\n            \"type\": \"str\",\\n            \"default\": \"Anger\"\\n        }\\n    }\\n}\\n\\n{\\n    \"name\": \"get_types\",\\n    \"description\": \"Fetches available types for a specified period from the horoscopes API.\",\\n    \"parameters\": {\\n        \"period\": {\\n            \"description\": \"The time period for which to retrieve the available types (e.g., daily, weekly, monthly).\",\\n            \"type\": \"str\",\\n            \"default\": \"\"\\n        }\\n    }\\n}\\n\\nWhat are the available types of horoscopes for this week and what wisdom quotes are related to motivation?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n[{\"name\": \"get_types\", \"arguments\": {\"period\": \"weekly\"}}, {\"name\": \"get_wisdom_quotes_by_topic\", \"arguments\": {\"topic\": \"motivation\"}}]<|eot_id|>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_calling_train[1][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wTZaJ8Y0vEX5",
    "outputId": "fe256f10-fb0e-4819-ba32-0f8e06841b3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Environment: ipython\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Given the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\n",
      "\n",
      "Respond in the format {\"name\": function name, \"arguments\": dictionary of argument name and its value}.Do not use variables.\n",
      "\n",
      "{\n",
      "    \"name\": \"get_wisdom_quotes_by_topic\",\n",
      "    \"description\": \"Fetches wisdom quotes related to a specific topic using the RapidAPI service.\",\n",
      "    \"parameters\": {\n",
      "        \"topic\": {\n",
      "            \"description\": \"The topic to retrieve wisdom quotes about.\",\n",
      "            \"type\": \"str\",\n",
      "            \"default\": \"Anger\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "{\n",
      "    \"name\": \"get_types\",\n",
      "    \"description\": \"Fetches available types for a specified period from the horoscopes API.\",\n",
      "    \"parameters\": {\n",
      "        \"period\": {\n",
      "            \"description\": \"The time period for which to retrieve the available types (e.g., daily, weekly, monthly).\",\n",
      "            \"type\": \"str\",\n",
      "            \"default\": \"\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "What are the available types of horoscopes for this week and what wisdom quotes are related to motivation?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "[{\"name\": \"get_types\", \"arguments\": {\"period\": \"weekly\"}}, {\"name\": \"get_wisdom_quotes_by_topic\", \"arguments\": {\"topic\": \"motivation\"}}]<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(function_calling_train[1][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Zb2kc4b2v8Zt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7oXbSw3dv8fH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fCb2xgneWPq3"
   },
   "outputs": [],
   "source": [
    "def formatting_instruction_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    if inputs:\n",
    "        convos = [{\"role\":\"user\",\"content\":instructions + \" \" + inputs},{\"role\":\"assistant\",\"content\":outputs}]\n",
    "    else:\n",
    "        convos = [{\"role\":\"user\",\"content\":instructions},{\"role\":\"assistant\",\"content\":outputs}]\n",
    "\n",
    "    texts = tokenizer.apply_chat_template(convos,tokenize = False, add_generation_prompt = False)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "\n",
    "dataset_instruction = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
    "dataset_instruction = dataset_instruction.map(formatting_instruction_prompts_func, batched = False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "S_mZ-40bJIkh"
   },
   "outputs": [],
   "source": [
    "dataset_instruction = dataset_instruction.select(insruction_indices)\n",
    "dataset_instruction = dataset_instruction.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "q085Cbj4b_7K"
   },
   "outputs": [],
   "source": [
    "NUM_SAMPLES_INSTRUCT_TRAIN = int(NUM_SAMPLES_INSTRUCT * 0.9)\n",
    "\n",
    "instruction_train = dataset_instruction.select(range(NUM_SAMPLES_INSTRUCT_TRAIN))\n",
    "instruction_validation = dataset_instruction.select(range(NUM_SAMPLES_INSTRUCT_TRAIN, NUM_SAMPLES_INSTRUCT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "IWk7rjnwJxo1"
   },
   "outputs": [],
   "source": [
    "train_dataset = concatenate_datasets([function_calling_train, instruction_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "-g2TJc03PanH"
   },
   "outputs": [],
   "source": [
    "validation_set = concatenate_datasets([function_calling_validation, instruction_validation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "K8iznn7wbr4Y"
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(seed=42)\n",
    "validation_set = validation_set.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "kb28DQiMgPSr",
    "outputId": "b0f325ba-f77b-48a0-cbcf-e09436e88452"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nEnvironment: ipython\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nGiven the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\\n\\nRespond in the format {\"name\": function name, \"arguments\": dictionary of argument name and its value}.Do not use variables.\\n\\n{\\n    \"name\": \"can_attend_all_meetings\",\\n    \"description\": \"Determines if a person can attend all meetings given a list of meeting time intervals.\",\\n    \"parameters\": {\\n        \"intervals\": {\\n            \"description\": \"A list of meeting time intervals, where each interval is represented as [start_time, end_time].\",\\n            \"type\": \"List[List[int]]\"\\n        }\\n    }\\n}\\n\\n{\\n    \"name\": \"fibonacci_numbers\",\\n    \"description\": \"Generates the first n Fibonacci numbers.\",\\n    \"parameters\": {\\n        \"n\": {\\n            \"description\": \"The number of Fibonacci numbers to generate.\",\\n            \"type\": \"int\"\\n        }\\n    }\\n}\\n\\nWhat are the first ten Fibonacci numbers?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n[{\"name\": \"fibonacci_numbers\", \"arguments\": {\"n\": 10}}]<|eot_id|>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_set[\"text\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "_3eqwTakygSW"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "_ZxZdAsO0FjA"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "wandb.init(mode=\"disabled\")\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "#os.environ[\"UNSLOTH_RETURN_LOGITS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "0e267abf-e18d-4981-e83b-a4d511961252"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA A100-SXM4-40GB. Max memory = 39.557 GB.\n",
      "5.748 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eKbZx2oFxgnB",
    "outputId": "387fd110-cb52-4392-81c0-f98d2678a7ea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = validation_set,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 32,\n",
    "        per_device_eval_batch_size = 8,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        batch_eval_metrics=True,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        learning_rate = 1e-4,\n",
    "        fp16 = True,#not is_bfloat16_supported(),\n",
    "        bf16 = False,#is_bfloat16_supported(),\n",
    "        logging_steps = 100,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"train_output\",\n",
    "        report_to=\"none\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 274
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "1aa5333a-a43a-4370-f2a6-d4a448fa439e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 11,430 | Num Epochs = 1 | Total steps = 358\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (32 x 1 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='358' max='358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [358/358 1:03:24, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.214000</td>\n",
       "      <td>0.159512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.158300</td>\n",
       "      <td>0.155043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.156600</td>\n",
       "      <td>0.153195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5NalxMkvjUtM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YY3zg3kq3NMa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upcOlWe7A1vc",
    "outputId": "83cdf25c-492f-4f76-b3ce-1ae79fc1bb13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/content/drive/MyDrive/Private code/Function calling LLM - EN/model_weights/tokenizer_config.json',\n",
       " '/content/drive/MyDrive/Private code/Function calling LLM - EN/model_weights/special_tokens_map.json',\n",
       " '/content/drive/MyDrive/Private code/Function calling LLM - EN/model_weights/tokenizer.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"model_weights\"\n",
    "\n",
    "model.save_pretrained(path) # Local saving\n",
    "tokenizer.save_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsAxDz3kw7Jf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m_6CkkJ3w7Lf",
    "outputId": "6e8b7785-7245-428a-a9b7-56fea7d6384b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.18: Fast Llama patching. Transformers: 4.50.0.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"model_weights\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = torch.half,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "KB3u9Dqgt4M4"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMYEjmMjK0as"
   },
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nIitok31XI9B",
    "outputId": "0daa8216-4a8e-478a-f6ff-4d95a2c6a5ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the integral of cos(x)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The integral of cos(x) is sin(x).<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"What is the integral of cos(x)\"\n",
    "convos = [{\"role\":\"user\",\"content\":instruction}]\n",
    "texts = tokenizer.apply_chat_template(convos,tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "inputs = tokenizer(\n",
    "[    texts\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = False)\n",
    "outputs = model.generate(**inputs, max_new_tokens = 4096, streamer = text_streamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M4k9PkNcXI_q",
    "outputId": "bbb4fafe-631a-4358-cdba-269d25ad72c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query', 'answers', 'tools', 'text'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_calling_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "ie0IISrduQJr"
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "query = function_calling_validation[\"query\"][index]\n",
    "convos = [{\"role\":\"user\",\"content\":query}]\n",
    "tool_object = json.loads(function_calling_validation[\"tools\"][index])\n",
    "texts = tokenizer.apply_chat_template(convos,tools=tool_object,tokenize = False, add_generation_prompt = False)\n",
    "texts = texts.replace('\"parameters\": d','\"arguments\": d') # original tool use function uses parameters our dataset uses arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w7WPqYHOK0cs",
    "outputId": "3d5147e5-5f30-40d1-bf23-bc1959a00b0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"name\": \"calculate_angle\", \"arguments\": {\"hour\": 11, \"minute\": 59}}]<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "convos = [{\"role\":\"user\",\"content\":texts}]\n",
    "texts = tokenizer.apply_chat_template(convos,tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "inputs = tokenizer(\n",
    "[    texts\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "outputs = model.generate(**inputs, max_new_tokens = 4096, streamer = text_streamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ROLcJcZ7vTv0",
    "outputId": "98c5f634-acfd-43ac-e2e0-aa2adcd6cbc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"name\": \"calculate_angle\", \"arguments\": {\"hour\": 11, \"minute\": 59}}]\n"
     ]
    }
   ],
   "source": [
    "print(function_calling_validation[index][\"answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s9nz1okkpgkR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JHwZ0Rmxvxa8",
    "outputId": "ce1798a3-bb02-482d-e09c-8e135ccf8e3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"name\": \"structural_analysis\", \"arguments\": {\"building_id\": \"B2\", \"floor_numbers\": [4, 5, 6], \"analysis_mode\": \"dynamic\"}}]<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "query = function_calling_validation[\"query\"][index]\n",
    "convos = [{\"role\":\"user\",\"content\":query}]\n",
    "tool_object = json.loads(function_calling_validation[\"tools\"][index])\n",
    "texts = tokenizer.apply_chat_template(convos,tools=tool_object,tokenize = False, add_generation_prompt = False)\n",
    "texts = texts.replace('\"parameters\": d','\"arguments\": d') # original tool use function uses parameters our dataset uses arguments\n",
    "convos = [{\"role\":\"user\",\"content\":texts}]\n",
    "texts = tokenizer.apply_chat_template(convos,tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "inputs = tokenizer(\n",
    "[    texts\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "outputs = model.generate(**inputs, max_new_tokens = 4096, streamer = text_streamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0UnTWg0yvzlS",
    "outputId": "a3a6e281-496a-4f42-a7a0-b73b1b1b4e93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"name\": \"structural_analysis\", \"arguments\": {\"building_id\": \"B2\", \"floor_numbers\": [4, 5, 6], \"analysis_mode\": \"dynamic\"}}]\n"
     ]
    }
   ],
   "source": [
    "print(function_calling_validation[index][\"answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "404Yr0CEv1WM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
